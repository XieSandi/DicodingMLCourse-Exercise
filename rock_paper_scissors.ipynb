{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " rock paper scissors.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqxgzBCv0NlV76Girg8uVS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XieSandi/DicodingMLCourse-Exercise/blob/main/rock_paper_scissors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xbq7BSnjuq2"
      },
      "source": [
        "**Load Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0BiLjc-UrIX"
      },
      "source": [
        "Jalankan perintah !wget berikut untuk mendownload dataset ke dalam temporary drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c06bcshOI81R"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "  https://dicodingacademy.blob.core.windows.net/picodiploma/ml_pemula_academy/rockpaperscissors.zip \\\n",
        "  -O /tmp/rockpaperscissors.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W5pRUukZZLw"
      },
      "source": [
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrnncFlhkGO5"
      },
      "source": [
        "\n",
        "**Installing Library / Package**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugs03TYxjXs5"
      },
      "source": [
        "pip install split-folders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MlFV0ICZYBj"
      },
      "source": [
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbhOCUPBkU-y"
      },
      "source": [
        "**Importing Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5gjNfg9jnct"
      },
      "source": [
        "import tensorflow as tf\n",
        "import splitfolders\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4WGCFfyZWum"
      },
      "source": [
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW6p2R_NmkSt"
      },
      "source": [
        "**Exctracting dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLoDCXKeZFe2"
      },
      "source": [
        "Silahkan sesuaikan dengan folder di drive anda , jika menyimpan file dataset di tempat berbeda (tidak melalui command wget diatas)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya75BD5vKPgB"
      },
      "source": [
        "local_zip = '/tmp/rockpaperscissors.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrHfi-7-lCTe"
      },
      "source": [
        "os.listdir('/tmp/rockpaperscissors')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf0OnTCGZbfk"
      },
      "source": [
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nwYu_MvlQZU"
      },
      "source": [
        "**Spliting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlNZBalVZuEW"
      },
      "source": [
        "Silahkan sesuaikan dengan link folder tempat anda melakukan perintah extract (jika berbeda)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NemMI8XMT3-Q"
      },
      "source": [
        "main_dir = \"/tmp/rockpaperscissors/rps-cv-images\"\n",
        "splitfolders.ratio(main_dir , output=\"/tmp/rockpaperscissors\", seed=1337, ratio=(.6, .4), group_prefix=None)\n",
        "\n",
        "train_dir = os.path.join('/tmp/rockpaperscissors/', 'train')\n",
        "validate_dir = os.path.join('/tmp/rockpaperscissors/', 'val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBE-V4F6ZoPd"
      },
      "source": [
        "."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3nvAOjdZktj"
      },
      "source": [
        "**Trainiing & Validating**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_qZrKm5NWj_"
      },
      "source": [
        "# training\n",
        "train_paper = os.path.join(train_dir, 'paper')\n",
        "train_rock = os.path.join(train_dir, 'rock')\n",
        "train_scissor = os.path.join(train_dir, 'scissor')\n",
        " \n",
        "#validating\n",
        "validation_paper = os.path.join(validate_dir, 'paper')\n",
        "validation_rock = os.path.join(validate_dir, 'rock')\n",
        "validation_scissor = os.path.join(validate_dir, 'scissor')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWVoGHIUO1Fn"
      },
      "source": [
        " train_datagen = ImageDataGenerator(\n",
        "                    rescale=1./255,\n",
        "                    rotation_range=20,\n",
        "                    horizontal_flip=True,\n",
        "                    shear_range = 0.2,\n",
        "                    fill_mode = 'nearest')\n",
        " \n",
        "test_datagen = ImageDataGenerator(\n",
        "                    rescale=1./255,\n",
        "                    rotation_range=20,\n",
        "                    horizontal_flip=True,\n",
        "                    shear_range = 0.2,\n",
        "                    fill_mode = 'nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PPbywqFPHkR"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        " \n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validate_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEO-0_CIW8FU"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aXQR_WiW-i3"
      },
      "source": [
        "# compile model with 'adam' optimizer loss function 'binary_crossentropy' \n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# latih model with model.fit \n",
        "model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=25,\n",
        "      epochs=20,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=5,\n",
        "      verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx8oSS8gYRUd"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "uploaded = files.upload()\n",
        " \n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = fn\n",
        "  img = image.load_img(path, target_size=(150, 150))\n",
        "  imgplot = plt.imshow(img)\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  \n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  \n",
        "  #String output template\n",
        "  paper     = \"[[1. 0. 0.]]\"\n",
        "  rock      = \"[[0. 1. 0.]]\"\n",
        "  scissors  = \"[[0. 0. 1.]]\"\n",
        "  \n",
        "  #convert numpy array into string\n",
        "  result = np.array_str(classes) \n",
        "  \n",
        "  \n",
        "  print('\\n')\n",
        "  print('Predict RESULT')\n",
        "  print(fn)\n",
        "  print(result)\n",
        "\n",
        "\n",
        "  #Changing result into human language\n",
        "  if result == paper :\n",
        "     print(\"paper\")\n",
        "  elif result == rock :\n",
        "     print(\"rock\")\n",
        "  elif result == scissors :\n",
        "     print(\"scissors\")\n",
        "  else :\n",
        "     print(\"[[%PAPER. %ROCK. %SCISSORS]]\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}