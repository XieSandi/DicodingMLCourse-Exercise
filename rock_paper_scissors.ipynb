{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " rock paper scissors.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7SCdUWVpw9+pPLdx5JKC+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XieSandi/DicodingMLCourse-Exercise/blob/main/rock_paper_scissors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xbq7BSnjuq2"
      },
      "source": [
        "**Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c06bcshOI81R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9956f941-c5e6-4eca-e78e-d90ea21453da"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "  https://dicodingacademy.blob.core.windows.net/picodiploma/ml_pemula_academy/rockpaperscissors.zip \\\n",
        "  -O /tmp/rockpaperscissors.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-12 19:53:09--  https://dicodingacademy.blob.core.windows.net/picodiploma/ml_pemula_academy/rockpaperscissors.zip\n",
            "Resolving dicodingacademy.blob.core.windows.net (dicodingacademy.blob.core.windows.net)... 52.239.197.36\n",
            "Connecting to dicodingacademy.blob.core.windows.net (dicodingacademy.blob.core.windows.net)|52.239.197.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 322873683 (308M) [application/zip]\n",
            "Saving to: ‘/tmp/rockpaperscissors.zip’\n",
            "\n",
            "/tmp/rockpapersciss 100%[===================>] 307.92M   724KB/s    in 4m 13s  \n",
            "\n",
            "2021-04-12 19:57:23 (1.22 MB/s) - ‘/tmp/rockpaperscissors.zip’ saved [322873683/322873683]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrnncFlhkGO5"
      },
      "source": [
        "**Installing Library / Package**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugs03TYxjXs5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37bac1c1-1b36-45f2-8204-d08f4cceeff5"
      },
      "source": [
        "pip install split-folders"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting split-folders\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/5f/3c2b2f7ea5e047c8cdc3bb00ae582c5438fcdbbedcc23b3cc1c2c7aae642/split_folders-0.4.3-py3-none-any.whl\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbhOCUPBkU-y"
      },
      "source": [
        "**Importing Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5gjNfg9jnct"
      },
      "source": [
        "import tensorflow as tf\n",
        "import splitfolders\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW6p2R_NmkSt"
      },
      "source": [
        "**Exctracting dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya75BD5vKPgB"
      },
      "source": [
        "local_zip = '/tmp/rockpaperscissors.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrHfi-7-lCTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb49d27e-0c65-4f09-bc44-89ca7a82b730"
      },
      "source": [
        "os.listdir('/tmp/rockpaperscissors')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['paper', 'scissors', 'rock', 'README_rpc-cv-images.txt', 'rps-cv-images']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nwYu_MvlQZU"
      },
      "source": [
        "**Spliting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NemMI8XMT3-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ec2a31-ec85-4e93-db8f-103915331d37"
      },
      "source": [
        "main_dir = \"/tmp/rockpaperscissors/rps-cv-images\"\n",
        "splitfolders.ratio(main_dir , output=\"/tmp/rockpaperscissors\", seed=1337, ratio=(.6, .4), group_prefix=None)\n",
        "\n",
        "train_dir = os.path.join('/tmp/rockpaperscissors/', 'train')\n",
        "validate_dir = os.path.join('/tmp/rockpaperscissors/', 'val')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying files: 2188 files [00:00, 4014.08 files/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_qZrKm5NWj_"
      },
      "source": [
        "# training\n",
        "train_paper = os.path.join(train_dir, 'paper')\n",
        "train_rock = os.path.join(train_dir, 'rock')\n",
        "train_scissor = os.path.join(train_dir, 'scissor')\n",
        " \n",
        "#validating\n",
        "validation_paper = os.path.join(validate_dir, 'paper')\n",
        "validation_rock = os.path.join(validate_dir, 'rock')\n",
        "validation_scissor = os.path.join(validate_dir, 'scissor')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWVoGHIUO1Fn"
      },
      "source": [
        " train_datagen = ImageDataGenerator(\n",
        "                    rescale=1./255,\n",
        "                    rotation_range=20,\n",
        "                    horizontal_flip=True,\n",
        "                    shear_range = 0.2,\n",
        "                    fill_mode = 'nearest')\n",
        " \n",
        "test_datagen = ImageDataGenerator(\n",
        "                    rescale=1./255,\n",
        "                    rotation_range=20,\n",
        "                    horizontal_flip=True,\n",
        "                    shear_range = 0.2,\n",
        "                    fill_mode = 'nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PPbywqFPHkR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4af80015-7336-4ae4-adbe-27b218fe6e69"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        " \n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validate_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1312 images belonging to 3 classes.\n",
            "Found 876 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEO-0_CIW8FU"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aXQR_WiW-i3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0941106e-81bd-4e45-fc40-190b048b4695"
      },
      "source": [
        "# compile model with 'adam' optimizer loss function 'binary_crossentropy' \n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# latih model with model.fit \n",
        "model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=25,\n",
        "      epochs=20,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=5,\n",
        "      verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "25/25 - 43s - loss: 0.1485 - accuracy: 0.9250 - val_loss: 0.1532 - val_accuracy: 0.9250\n",
            "Epoch 2/20\n",
            "25/25 - 42s - loss: 0.1319 - accuracy: 0.9200 - val_loss: 0.1922 - val_accuracy: 0.9000\n",
            "Epoch 3/20\n",
            "25/25 - 42s - loss: 0.1176 - accuracy: 0.9525 - val_loss: 0.1395 - val_accuracy: 0.9250\n",
            "Epoch 4/20\n",
            "25/25 - 42s - loss: 0.0737 - accuracy: 0.9500 - val_loss: 0.1281 - val_accuracy: 0.9625\n",
            "Epoch 5/20\n",
            "25/25 - 42s - loss: 0.0776 - accuracy: 0.9638 - val_loss: 0.0594 - val_accuracy: 0.9688\n",
            "Epoch 6/20\n",
            "25/25 - 42s - loss: 0.0642 - accuracy: 0.9638 - val_loss: 0.1893 - val_accuracy: 0.9312\n",
            "Epoch 7/20\n",
            "25/25 - 42s - loss: 0.0623 - accuracy: 0.9613 - val_loss: 0.0938 - val_accuracy: 0.9563\n",
            "Epoch 8/20\n",
            "25/25 - 42s - loss: 0.0502 - accuracy: 0.9775 - val_loss: 0.1337 - val_accuracy: 0.9500\n",
            "Epoch 9/20\n",
            "25/25 - 42s - loss: 0.0494 - accuracy: 0.9775 - val_loss: 0.0843 - val_accuracy: 0.9688\n",
            "Epoch 10/20\n",
            "25/25 - 42s - loss: 0.0625 - accuracy: 0.9675 - val_loss: 0.1187 - val_accuracy: 0.9375\n",
            "Epoch 11/20\n",
            "25/25 - 42s - loss: 0.0378 - accuracy: 0.9900 - val_loss: 0.0411 - val_accuracy: 0.9812\n",
            "Epoch 12/20\n",
            "25/25 - 42s - loss: 0.0453 - accuracy: 0.9812 - val_loss: 0.0239 - val_accuracy: 0.9875\n",
            "Epoch 13/20\n",
            "25/25 - 42s - loss: 0.0665 - accuracy: 0.9675 - val_loss: 0.0796 - val_accuracy: 0.9625\n",
            "Epoch 14/20\n",
            "25/25 - 42s - loss: 0.0341 - accuracy: 0.9812 - val_loss: 0.0255 - val_accuracy: 0.9937\n",
            "Epoch 15/20\n",
            "25/25 - 42s - loss: 0.0163 - accuracy: 0.9912 - val_loss: 0.1513 - val_accuracy: 0.9563\n",
            "Epoch 16/20\n",
            "25/25 - 42s - loss: 0.0231 - accuracy: 0.9900 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "25/25 - 42s - loss: 0.0128 - accuracy: 0.9950 - val_loss: 0.0840 - val_accuracy: 0.9625\n",
            "Epoch 18/20\n",
            "25/25 - 42s - loss: 0.0200 - accuracy: 0.9900 - val_loss: 0.0138 - val_accuracy: 0.9937\n",
            "Epoch 19/20\n",
            "25/25 - 42s - loss: 0.0229 - accuracy: 0.9850 - val_loss: 0.0559 - val_accuracy: 0.9937\n",
            "Epoch 20/20\n",
            "25/25 - 42s - loss: 0.0147 - accuracy: 0.9950 - val_loss: 0.0355 - val_accuracy: 0.9875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f539198df10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx8oSS8gYRUd"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "uploaded = files.upload()\n",
        " \n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = fn\n",
        "  img = image.load_img(path, target_size=(150, 150))\n",
        "  imgplot = plt.imshow(img)\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  \n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  \n",
        "  #String output template\n",
        "  paper     = \"[[1. 0. 0.]]\"\n",
        "  rock      = \"[[0. 1. 0.]]\"\n",
        "  scissors  = \"[[0. 0. 1.]]\"\n",
        "  \n",
        "  #convert numpy array into string\n",
        "  result = np.array_str(classes) \n",
        "  \n",
        "  \n",
        "  print('\\n')\n",
        "  print('Predict RESULT')\n",
        "  print(fn)\n",
        "  print(result)\n",
        "\n",
        "\n",
        "  #Changing result into human language\n",
        "  if result == paper :\n",
        "     print(\"paper\")\n",
        "  elif result == rock :\n",
        "     print(\"rock\")\n",
        "  elif result == scissors :\n",
        "     print(\"scissors\")\n",
        "  else :\n",
        "     print(\"[[%PAPER. %ROCK. %SCISSORS]]\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}